{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Founded',\n",
       " 'in',\n",
       " '2002',\n",
       " ',',\n",
       " 'Spacex',\n",
       " \"'s\",\n",
       " 'mission',\n",
       " 'is',\n",
       " 'to',\n",
       " 'enable',\n",
       " 'humans',\n",
       " 'to',\n",
       " 'become',\n",
       " 'a',\n",
       " 'spacefaring',\n",
       " 'civilization',\n",
       " 'and',\n",
       " 'a',\n",
       " 'multi-planet',\n",
       " 'species',\n",
       " 'by',\n",
       " 'building',\n",
       " 'a',\n",
       " 'self-sustaining',\n",
       " 'city',\n",
       " 'on',\n",
       " 'mars',\n",
       " '.']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Tokenization using NLTK \n",
    "# Word Tokenization\n",
    "from nltk.tokenize import word_tokenize\n",
    "text=\"\"\"Founded in 2002,Spacex's mission is to enable humans to become a spacefaring civilization and a multi-planet species by building a self-sustaining city on mars.\"\"\"\n",
    "word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sakshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Founded in 2002,Spacex's mission is to enable humans to become a spacefaring civilization and a multi-planet species by building a self-sustaining city on mars.In 2008,Spacex's Falcon 1 became the first privately developed liquid-fuel launch vehicle to orbit the Earth\"]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Tokenization using NLTK \n",
    "from nltk.tokenize import sent_tokenize\n",
    "sentence=\"\"\"Founded in 2002,Spacex's mission is to enable humans to become a spacefaring civilization and a multi-planet species by building a self-sustaining city on mars.In 2008,Spacex's Falcon 1 became the first privately developed liquid-fuel launch vehicle to orbit the Earth\"\"\"\n",
    "sent_tokenize(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sakshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmer: seen\n",
      "Lemmatizer: see\n",
      "\n",
      "Stemmer: drove\n",
      "Lemmatizer: drive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def compare_stemmer_lemmatizer(stemmer, lemmatizer, word, pos):\n",
    "    \"\"\"\n",
    "    Print the results of stemmind and lemmitization using the passed stemmer, lemmatizer, word and pos (part of speech)\n",
    "    \"\"\"\n",
    "    print(\"Stemmer:\", stemmer.stem(word))\n",
    "    print(\"Lemmatizer:\", lemmatizer.lemmatize(word, pos))\n",
    "    print()\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "compare_stemmer_lemmatizer(stemmer, lemmatizer, word = \"seen\", pos = wordnet.VERB)\n",
    "compare_stemmer_lemmatizer(stemmer, lemmatizer, word = \"drove\", pos = wordnet.VERB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sakshi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'JJ'),\n",
       " ('language', 'NN'),\n",
       " ('processing', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('complex', 'JJ'),\n",
       " ('field', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('intersection', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('Artificial', 'NNP'),\n",
       " ('Intelligence', 'NNP'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('computer', 'NN'),\n",
       " ('science', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text =\"Natural language processing is a complex field and is the intersection of Artificial Intelligence,and computer science.\"\n",
    "text1=word_tokenize(text)\n",
    "tags=nltk.pos_tag(text1)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Python', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('widely', 'RB'),\n",
       " ('used', 'VBN'),\n",
       " ('Programming', 'NNP'),\n",
       " ('language', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text =\"Python is widely used Programming language.\"\n",
    "text1=word_tokenize(text)\n",
    "tags=nltk.pos_tag(text1)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammar = \"NP:{<DT>?<JJ>*<NN>}\" \n",
    "Reg_parser = nltk.RegexpParser(grammar)\n",
    "Reg_parser.parse(tags)\n",
    "Output = Reg_parser.parse(tags)\n",
    "Output.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   city  developing  growing  live  love  pune  rapidly\n",
      "0     2           1        1     0     0     1        1\n",
      "1     0           0        0     1     1     1        0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    " \n",
    "sentence_1=\"Pune city is growing and rapidly developing city.  \"\n",
    "sentence_2=\"I would love to live in Pune.\"\n",
    " \n",
    " \n",
    " \n",
    "CountVec = CountVectorizer(ngram_range=(1,1), # to use bigrams ngram_range=(2,2)\n",
    "                           stop_words='english')\n",
    "#transform\n",
    "Count_data = CountVec.fit_transform([sentence_1,sentence_2])\n",
    " \n",
    "#create dataframe\n",
    "cv_dataframe=pd.DataFrame(Count_data.toarray(),columns=CountVec.get_feature_names())\n",
    "print(cv_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<zip at 0x12778a7a1c0>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "from collections import Counter\n",
    "text = \"Absolutely wonderful - silky and sexy and comfortable.\"\n",
    "one_grams = ngrams(nltk.word_tokenize(text), 1)\n",
    "one_grams\n",
    "Two_grams = ngrams(nltk.word_tokenize(text), 2)\n",
    "Two_grams\n",
    "Three_grams = ngrams(nltk.word_tokenize(text), 3)\n",
    "Three_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_tweets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-4b0593ba404e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature_extraction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m data = {'twitter':get_tweets(),\n\u001b[0m\u001b[0;32m      4\u001b[0m         'facebook':get_fb_statuses()}\n\u001b[0;32m      5\u001b[0m \u001b[0mvectoriser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'get_tweets' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "data = {'twitter':get_tweets(),\n",
    "        'facebook':get_fb_statuses()}\n",
    "vectoriser = TfidfVectorizer()\n",
    "vec = vectoriser.fit_transform(data['twitter'].append(data['facebook']))\n",
    "df = pd.DataFrame(vec.toarray().transpose(), index = vectoriser.get_feature_names())\n",
    "df.columns = ['twitter', 'facebook']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Absolutely',)\n",
      "('wonderful',)\n",
      "('-',)\n",
      "('silky',)\n",
      "('and',)\n",
      "('sexy',)\n",
      "('and',)\n",
      "('comfortable.',)\n"
     ]
    }
   ],
   "source": [
    "from nltk import ngrams\n",
    " \n",
    "sentence = \"Absolutely wonderful - silky and sexy and comfortable.\"\n",
    "ngram = ngrams(sentence.split(' '), n=1)\n",
    "for x in ngram:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Absolutely', 'wonderful')\n",
      "('wonderful', '-')\n",
      "('-', 'silky')\n",
      "('silky', 'and')\n",
      "('and', 'sexy')\n",
      "('sexy', 'and')\n",
      "('and', 'comfortable.')\n"
     ]
    }
   ],
   "source": [
    "ngram = ngrams(sentence.split(' '), n=2)\n",
    "for x in ngram:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Absolutely', 'wonderful', '-')\n",
      "('wonderful', '-', 'silky')\n",
      "('-', 'silky', 'and')\n",
      "('silky', 'and', 'sexy')\n",
      "('and', 'sexy', 'and')\n",
      "('sexy', 'and', 'comfortable.')\n"
     ]
    }
   ],
   "source": [
    "ngram = ngrams(sentence.split(' '), n=3)\n",
    "for x in ngram:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary       and  antagonistic       are      cats      dogs      four  \\\n",
      "sentence1   0.000000      0.000000  0.000000  0.379978  0.000000  0.534046   \n",
      "sentence2   0.471078      0.471078  0.471078  0.335176  0.471078  0.000000   \n",
      "\n",
      "vocabulary      have      legs  \n",
      "sentence1   0.534046  0.534046  \n",
      "sentence2   0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    " \n",
    "corpus = ['Cats have four legs',\n",
    "          'Cats and dogs are antagonistic']\n",
    " \n",
    "tfidf = TfidfVectorizer()\n",
    "vect = tfidf.fit_transform(corpus)\n",
    " \n",
    "df = pd.DataFrame()\n",
    "df['vocabulary'] = tfidf.get_feature_names()\n",
    "df['sentence1'] = vect.toarray()[0]\n",
    "df['sentence2'] = vect.toarray()[1]\n",
    "df.set_index('vocabulary', inplace=True)\n",
    "print(df.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
